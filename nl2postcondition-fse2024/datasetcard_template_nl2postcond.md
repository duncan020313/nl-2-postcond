# Dataset Card for nl2postcond (natural language to declarative specifications)

This repository contains the replication materials for the paper to evaluate LLM's ability to generate declarative formal specifications from informal/natural language intent.

   *Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?* by Endres, Fakhoury, Chakraborty, Lahiri, to appear at FSE'24. A preprint of the paper is available here [https://arxiv.org/abs/2310.01831](https://arxiv.org/abs/2310.01831)


## Dataset Details

This repository contains the following:

- All LLM prompts and postconditions generated across different LLMs (GPT-3.5, GPT-4, StarCoder) for the EvalPlus benchmark
- The set of code-mutants for EvalPlus generated by querying LLM (GPT-3.5) 
- Qualitative analysis spreadsheet Analysis scripts 
- Docker container containing scripts for evaluating LLM generated postconditions for EvalPlus benchmark
- LLM prompts and postconditions generated for the Defects4J benchmark 


### Dataset Description

Please refer to "Dataset Structure" for more information of the dataset. 

### Dataset Sources 

We adapt the [EvalPlus code generation dataset](https://github.com/evalplus/evalplus) for the task of specification generation and evaluation. We adapt the scripts to evaluate generated specifications according to the correctness and completeness metrics as defined in the FSE'24 paper. 

We also leverage the [Defects4J bug dataset](https://github.com/rjust/defects4j) to evaluate LLMs ability to generate bug-discriminating postconditions to discover (historical) bugs in real-world projects. 

## Uses


### Direct Use

The intended use of the dataset and evaluation scripts are (a) as a replication package for the results in the FSE'24 paper on nl2postcond, and (b) enable researchers to experiment with other specification generation (from informal intent) methods and evaluate them. 

### Out-of-Scope Use

Given that the LLM generated code and specifications can be buggy, the code mutants or the specifications generated should not be trusted. Besides, given that the evaluation scripts involve executing programs, they can have unintended effects. We recommend running these scripts inside the Docker containers (including the ones provided for EvalPlus). The quality of the specifications and code mutants may not generalize beyond the benchmarks considered in the paper. 


## Dataset Structure

The dataset is organized into subfolders in the [GitHub repository](https://github.com/microsoft/nl-2-postcond/tree/main/nl2postcondition-fse2024). 
Subfolders of this repository contains their own READMEs with more detailed instructions. The layout of this repository is:

* `GeneratedPostconditions` : All generated postconditions analyzed in the FSE paper, along with their evaluation results and logs. Includes both EvalPlus and Defects4J results.
* `QualitativeAnalysis` : A spreadsheet with the results of our manual analysis of a subset of EvalPlus postconditions
* `PromptTemplates` : Contains all prompts ablations used for both EvalPlus and Defects4J. 
* `nl2postcondition_source_evalplus`: All nl2postcondition code for the EvalPlus benchmark. Includes scripts for postcondition generation, postcondition preprocessing, and postcondition evaluation.

## Dataset Creation

### Curation Rationale

The dataset was curated to evaluate large langauge models (LLMs) ability to synthesize formal declarative specifications from informal intent. This included access to a corpus of natural language and ground truth formal intent (reference code and validation tests) for which we leverage EvalPlus. We define metrics for evaluating generating specifications that requires executing these specifications on reference and buggy code against the validation tests. We define various prompts and demonstrate their effects on the quality of generations. We also package the generated postconditions and code-mutants generated for other researchers to replicate and extend this line of research. 

### Source Data

We adapt the EvalPlus and Defects4J benchmarks for our task. References are listed above. 

#### Personal and Sensitive Information

The dataset consists of code comments (docstrings), code and tests, curated by earlier works on EvalPlus and Defects4J. It does not contain personal, sensitive, private data as far as we can tell.  

## Bias, Risks, and Limitations

Given that the LLM generated code and specifications can be buggy, the code mutants or the specifications generated should not be trusted. Besides, given that the evaluation scripts involve executing programs, they can have unintended effects. We recommend running these scripts inside the Docker containers (including the ones provided for EvalPlus). The quality of the specifications and code mutants may not generalize beyond the benchmarks considered in the paper. 


## Citation

**BibTeX:**

@misc{endres2024largelanguagemodelstransform,
      title={Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?}, 
      author={Madeline Endres and Sarah Fakhoury and Saikat Chakraborty and Shuvendu K. Lahiri},
      year={2024},
      eprint={2310.01831},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2310.01831}, 
}

## Dataset Card Contact

shuvendun AT microsoft DOT com [REMOVE the second n from the email name]
