# Generated Postconditions

This file contains all postconditions generated and evaluated for the paper *Can Large Language Models Transform Natural Language Intent*. This includes both the postconditions for EvalPlus and for Defects4J.

### EvalPlus

The `EvalPlus` sub-folder contains the data and analysis results for *GPT 3.5*, *GPT 4*, and *Star Chat*. For each model, we ran four prompt ablations:

1. Base prompt with reference code
2. Simple prompt with reference code
3. Base prompt without reference code
4. Simple prompt without reference code

Our paper contains more information about each prompt ablation. In this repository, we include a zip file associated with each ablation. Each zip file contains the following artifacts:

* `raw_preprocessed_samples.jsonl`: This file contains all LLM-generated samples post lightweight preprocessing via the response_preprocessing.py script. Each json item corresponds to an individual postcondition, and contains both the postcondition itself as well as the postcondition wrapped around its associated EvalPlus problem.
* `pass_at_k.txt`: Summary of the postcondition correctness results. These are the results reported in Table 1 of the paper.
* `soundness_eval_results_preprocessed_samples.jsonl`: File containing detailed completeness results for each test in EvalPlus for each postcondition.
* `power_eval_stats.txt`: A summary of the completeness results. A subset of the results in this file are reported in Table 2 in the paper.
* `individual_postcondition_results`: A folder that contains detailed completeness evaluation results for each distinguishing input and code for every postcondition.
* `summary_eval_results.jsonl`: Contains a programmatically readable summary of the correctness and completeness of each postcondition.
* `llmGenConfig_HydraCopy`: The hydra configuration files used to run a given ablation. We note that `prompt_v: 3` refers to the `base` prompt and `prompt_v: 5` refers to the `simple` prompt. In the code elsewhere in this repository, the prompt versions have been updated to the strings `simple` and `base` to reflect the terminology in the paper. 

### Defects4J

The `Defects4J` sub-folder contains postconditions generated by *GPT 4*, and *Star Chat*. For each model, we used two prompt ablations:

1. Buggy method reference code included
2. Buggy method reference code not included

Our paper contains more information about each prompt ablation. In this repository, we include a zip file associated with each ablation. Each zip file contains the following artifacts:

* `human_readable_programs`: All generated postconditions in a human-readable format (one file per postcondition). These postconditions are not yet preprocessed and are the raw model output. Postconditions for all modified methods in Defects4J are included. This includes some methods that were not included in our evaluation due to Java-related constraints.
* Jsonl files with programmatically readable raw output of the LLM
* `.hydra`: The hydra configuration files used to run a given ablation. 

We note that some of the zip files contain multiple sub-directories as the postconditions were generated in batches.



